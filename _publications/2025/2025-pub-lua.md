---
title:          "Lua-LLM: Learning Unstructured-Sparsity Allocation for Large Language Models"
date:           2025-09-19
selected:       true
pub:            "Advances in Neural Information Processing Systems (NeurIPS)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2025"
#semantic_scholar_id: 204e3073870fae3d05bcbc2f6a8e263d9b72e776  # use this to retrieve citation count
abstract: >-
  We propose Lua-LLM (Learning unstructured-sparsity allocation in LLMs), a learning-based global pruning framework that explores the optimal unstructured sparsity allocation. Unlike existing pruning methods, which primarily focus on allocating per-layer sparsity, Lua-LLM achieves flexible allocation for both layer-wise and intra-layer sparsity.
cover:          /assets/images/covers/end.png
authors:
  - <strong>Mingge Lu</strong>, Jingwei Sun, Junqing Lin, Zechun Zhou, Guangzhong Sun
links:
  #Code: https://github.com/luost26/academic-homepage
  Openreview: https://openreview.net/forum?id=CA1xVSvn72
---
